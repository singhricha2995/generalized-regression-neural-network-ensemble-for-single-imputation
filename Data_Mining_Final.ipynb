{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Mining_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA1dXIlqHX2j",
        "colab_type": "text"
      },
      "source": [
        "**Link with Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWigIgL88oKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d2abccc-cd3f-4ea7-c7a3-87b70484d38a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbcuIs6QNWrr",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx1OGg1PBS05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "from operator import itemgetter\n",
        "from tqdm import tqdm\n",
        "from scipy import optimize\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.gaussian_process import kernels\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0VDaHmhRKN",
        "colab_type": "text"
      },
      "source": [
        "**GLOBAL LOG VARIABLE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5NSwWiEhUYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgejeiIJNbCy",
        "colab_type": "text"
      },
      "source": [
        "**Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOONkgfCGyov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Establishing directoyires to read original as well as incomplete datasets\n",
        "def read_subsets_and_original(BASE_PATH, ORIGINAL_BASE_PATH, dataset_name):\n",
        "  sub_incomplete_dataset = os.listdir(BASE_PATH + dataset_name)\n",
        "  # print (\"Found Total Subsets : \", len(sub_incomplete_dataset))\n",
        "  original = pd.read_excel(ORIGINAL_BASE_PATH + dataset_name + '.xlsx', header=None)\n",
        "  original = original.infer_objects()\n",
        "  subsets = {}\n",
        "  for each in tqdm(sub_incomplete_dataset, total=len(sub_incomplete_dataset)):\n",
        "    subsets[each.split('.')[0]] = pd.read_excel(BASE_PATH + dataset_name + '/' + each, header=None)\n",
        "  return subsets, original\n",
        "\n",
        "#Calculate NRMS value (Formula according to the documentation)\n",
        "def calculate_NRMS(y_true, y_pred):\n",
        "  upper_values = y_pred - y_true\n",
        "  #CHECK DOCUMENTATION ON https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "\n",
        "  #ord = 'fro' means frobenius norm\n",
        "  upper_normed = np.linalg.norm(upper_values, ord='fro')\n",
        "  lower_normed = np.linalg.norm(y_true, ord='fro')\n",
        "  return upper_normed / lower_normed\n",
        "\n",
        "#Calculate AE value (Formula according to the documentation)\n",
        "def calculate_AE_DICT(y_true, y_pred):\n",
        "  y_true = y_true.flatten()\n",
        "  y_pred = y_pred.flatten()\n",
        "  return y_true[y_true == y_pred].shape[0] / float(y_true.shape[0]) #(1 0 1 0 1) #(0 1 0 0 1) #2/5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9RejF6E-0L",
        "colab_type": "text"
      },
      "source": [
        "**GRNN Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6yZXO9LE9sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build GRNN Model\n",
        "class GRNN(BaseEstimator, RegressorMixin):\n",
        "    #Initializing all the elements\n",
        "    def __init__(self, kernel='RBF', sigma=0.7, n_splits=5, calibration='warm_start', method='L-BFGS-B', bnds=(0, None), n_restarts_optimizer=0, seed = 42):\n",
        "        self.kernel = kernel\n",
        "        self.sigma = sigma\n",
        "        self.n_splits = n_splits\n",
        "        self.calibration = calibration\n",
        "        self.method = method\n",
        "        self.iterations = 0\n",
        "        self.bnds = bnds\n",
        "        self.n_restarts_optimizer = n_restarts_optimizer\n",
        "        self.seed = seed\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "\n",
        "        # Check that X and y have correct shape\n",
        "        # X, y = check_X_y(X, y)\n",
        "        \n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        bounds = self.bnds\n",
        "        \n",
        "        np.seterr(divide='ignore', invalid='ignore')\n",
        "        \n",
        "        #Initializaing and establishing the cost function\n",
        "        def cost(sigma_):\n",
        "            kf = KFold(n_splits= self.n_splits, random_state=self.seed)\n",
        "            kf.get_n_splits(self.X_)\n",
        "            cv_err = []\n",
        "            for train_index, validate_index in kf.split(self.X_):\n",
        "                X_tr, X_val = self.X_[train_index], self.X_[validate_index]\n",
        "                y_tr, y_val = self.y_[train_index], self.y_[validate_index]\n",
        "                Kernel_def_= getattr(kernels, self.kernel)(length_scale=sigma_)\n",
        "                K_ = Kernel_def_(X_tr, X_val)\n",
        "                # If the distances are very high/low, zero-densities must be prevented:\n",
        "                K_ = np.nan_to_num(K_)\n",
        "                psum_ = K_.sum(axis=0).T # Cumulate denominator of the Nadaraya-Watson estimator\n",
        "                psum_ = np.nan_to_num(psum_)\n",
        "                y_pred_ = (np.dot(y_tr.T, K_) / psum_)\n",
        "                y_pred_ = np.nan_to_num(y_pred_)\n",
        "                cv_err.append(MSE(y_val, y_pred_.T))\n",
        "                break\n",
        "            return cv_err[0] ## Mean error over the k splits                        \n",
        "        \n",
        "        #Establising the optimization function\n",
        "        def optimization(x0_):\n",
        "            rlog = \"\"\n",
        "            if len(self.bnds) > 1:\n",
        "              self.bnds = (self.bnds[0], )\n",
        "\n",
        "\n",
        "            try:\n",
        "              if len(x0_) > 1:\n",
        "                x0_ = x0_[0]\n",
        "            except:\n",
        "              rlog = \"x0_ is Good Enough\"\n",
        "\n",
        "            # print (\"x0_\", x0_)\n",
        "            # print (\"Bounds : \", self.bnds)\n",
        "            opt = optimize.minimize(cost, x0_, method=self.method, bounds=self.bnds)\n",
        "            if opt['success'] is True:\n",
        "                opt_sigma = opt['x']\n",
        "                opt_cv_error = opt['fun']\n",
        "            else:\n",
        "                opt_sigma = np.full(len(self.X_[0]), np.nan)\n",
        "                opt_cv_error = np.inf\n",
        "                pass\n",
        "            return [opt_sigma, opt_cv_error]\n",
        "        \n",
        "        #Regulating and calibrating sigma\n",
        "        def calibrate_sigma(self):\n",
        "            x0 = np.asarray(self.sigma) # Starting guess (either user-defined or measured with warm start)\n",
        "            if self.n_restarts_optimizer > 0:\n",
        "                # print (\"################################\")    \n",
        "                optima = [optimization(x0)]            \n",
        "                #First optimize starting from theta specified in kernel\n",
        "                optima = [optimization(x0)] \n",
        "                # # Additional runs are performed from log-uniform chosen initial bandwidths\n",
        "                r_s = np.random.RandomState(self.seed)\n",
        "                for iteration in range(self.n_restarts_optimizer): \n",
        "                    x0_iter = np.full(len(self.X_[0]), np.around(r_s.uniform(0,1), decimals=3))\n",
        "                    optima.append(optimization(x0_iter))             \n",
        "            elif self.n_restarts_optimizer == 0: \n",
        "                # print (\"Running SAD ONE\")    \n",
        "                optima = [optimization(x0)]            \n",
        "            else:\n",
        "                raise ValueError('n_restarts_optimizer must be a positive int!')\n",
        "            \n",
        "            # Select sigma from the run minimizing cost\n",
        "            cost_values = list(map(itemgetter(1), optima))\n",
        "            self.sigma = optima[np.argmin(cost_values)][0]\n",
        "            self.cv_error = np.min(cost_values) \n",
        "            return self\n",
        "        \n",
        "        global log\n",
        "        if self.calibration is 'warm_start':\n",
        "            log = log + 'Executing warm start...' + '/n'\n",
        "            self.bnds = (bounds,)           \n",
        "            x0 = np.asarray(self.sigma)\n",
        "            optima = [optimization(x0)]            \n",
        "            cost_values = list(map(itemgetter(1), optima))\n",
        "            self.sigma = optima[np.argmin(cost_values)][0]\n",
        "            log = log + 'Warm start concluded. The optimum isotropic sigma is ' + str(self.sigma) + '/n'\n",
        "            self.sigma = np.full(len(self.X_[0]), np.around(self.sigma, decimals=3))\n",
        "            self.bnds = (bounds,)*len(self.X_[0])\n",
        "            # print ('Executing gradient search...')\n",
        "            calibrate_sigma(self)\n",
        "            log = log + 'Gradient search concluded. The optimum sigma is ' + str(self.sigma) + '/n'\n",
        "        elif self.calibration is 'gradient_search':\n",
        "            #print ('Executing gradient search...')\n",
        "            self.sigma = np.full(len(self.X_[0]), self.sigma)\n",
        "            self.bnds = (bounds,)*len(self.X_[0])\n",
        "            calibrate_sigma(self)\n",
        "            #print('Gradient search concluded. The optimum sigma is ' + str(self.sigma))\n",
        "        else:\n",
        "            pass\n",
        "                   \n",
        "        self.is_fitted_ = True\n",
        "        # Return the regressor\n",
        "        return self\n",
        "\n",
        "    #Gathering all the above and predicting the values \n",
        "    def predict(self, X):\n",
        "        \n",
        "         # Check if fit had been called\n",
        "        # check_is_fitted(self, ['X_', 'y_'])\n",
        "        \n",
        "        # Input validation\n",
        "        X = check_array(X)\n",
        "        \n",
        "        Kernel_def= getattr(kernels, self.kernel)(length_scale=self.sigma)\n",
        "        K = Kernel_def(self.X_, X)\n",
        "        # If the distances are very high/low, zero-densities must be prevented:\n",
        "        K = np.nan_to_num(K)\n",
        "        psum = K.sum(axis=0).T # Cumulate denominator of the Nadaraya-Watson estimator\n",
        "        psum = np.nan_to_num(psum)\n",
        "        return np.nan_to_num((np.dot(self.y_.T, K) / psum))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-FuRaHEGHdw",
        "colab_type": "text"
      },
      "source": [
        "**SAGA Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iIBM2DvGNMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################\n",
        "############################################\n",
        "\n",
        "def SAGA_FEATURE_SELECTION(X_train, y_train):\n",
        "  model_logistic = Ridge(solver='saga')\n",
        "  sel_model_logistic = SelectFromModel(estimator=model_logistic)\n",
        "  X_train_sfm_l1 = sel_model_logistic.fit_transform(X_train.values, y_train.values)\n",
        "  Indicator_columns = sel_model_logistic.get_support()\n",
        "  return Indicator_columns #SAGA BASED FEATURE SELECTION\n",
        "\n",
        "############################################\n",
        "############################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpCYuhx_OAhN",
        "colab_type": "text"
      },
      "source": [
        "**Reading Metadata**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmIf03r2OAGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading the data to run our model on\n",
        "BASE = 'gdrive/My Drive/Course Project Datasets/Course Project Datasets/'\n",
        "BASE_PATH = BASE + 'Incomplete Datasets Without Labels/'\n",
        "ORIGINAL_BASE_PATH = BASE + 'Original Datasets Without Labels/'\n",
        "meta_data = pd.read_excel(BASE + 'List of Datasets.xlsx')\n",
        "categorical = meta_data[meta_data['Numerical'] == 0].reset_index(drop=True)\n",
        "numerical = meta_data[meta_data['Categorical'] == 0].reset_index(drop=True)\n",
        "combined = meta_data[(meta_data['Numerical'] != 0) & (meta_data['Categorical'] != 0)].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNYq7JuXNm10",
        "colab_type": "text"
      },
      "source": [
        "**Processing Datasets with only Numerical Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeGifsVZNjcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "09d1f2c0-4a50-4e86-88cc-33580b5727d8"
      },
      "source": [
        "numerical #Datasets with only Numerical Values\n",
        "numerical['Abbreviation'][numerical['Abbreviation'] == 'BUPA'] = 'Bupa'\n",
        "numerical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset Name</th>\n",
              "      <th>Abbreviation</th>\n",
              "      <th>Instances</th>\n",
              "      <th>Features</th>\n",
              "      <th>Classes</th>\n",
              "      <th>Numerical</th>\n",
              "      <th>Categorical</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Iris</td>\n",
              "      <td>Iris</td>\n",
              "      <td>150</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wine</td>\n",
              "      <td>Wine</td>\n",
              "      <td>178</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Glass</td>\n",
              "      <td>Glass</td>\n",
              "      <td>214</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Statlog Heart (CL)</td>\n",
              "      <td>Sheart</td>\n",
              "      <td>270</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BUPA liver disorders</td>\n",
              "      <td>Bupa</td>\n",
              "      <td>345</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ionosphere</td>\n",
              "      <td>Ionosphere</td>\n",
              "      <td>351</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Sonar</td>\n",
              "      <td>Sonar</td>\n",
              "      <td>208</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Four Gaussian</td>\n",
              "      <td>4-gauss</td>\n",
              "      <td>800</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Breast Cancer Wisconsin</td>\n",
              "      <td>BCW</td>\n",
              "      <td>683</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Pima Indians Diabetes</td>\n",
              "      <td>PID</td>\n",
              "      <td>768</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Dermatology</td>\n",
              "      <td>DERM</td>\n",
              "      <td>358</td>\n",
              "      <td>34</td>\n",
              "      <td>6</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Difficult Doughnut</td>\n",
              "      <td>Difdoug</td>\n",
              "      <td>400</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4-Class Noisy Pinwheel</td>\n",
              "      <td>CNP</td>\n",
              "      <td>4000</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Yeast</td>\n",
              "      <td>Yeast</td>\n",
              "      <td>1484</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Spam</td>\n",
              "      <td>Spam</td>\n",
              "      <td>4597</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Letter</td>\n",
              "      <td>Letter</td>\n",
              "      <td>20000</td>\n",
              "      <td>16</td>\n",
              "      <td>26</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Dataset Name Abbreviation  ...  Numerical  Categorical\n",
              "0                     Iris          Iris  ...          4            0\n",
              "1                      Wine         Wine  ...         13            0\n",
              "2                     Glass        Glass  ...          9            0\n",
              "3        Statlog Heart (CL)       Sheart  ...         13            0\n",
              "4      BUPA liver disorders         Bupa  ...          6            0\n",
              "5                Ionosphere   Ionosphere  ...         34            0\n",
              "6                     Sonar        Sonar  ...         60            0\n",
              "7            Four Gaussian       4-gauss  ...         12            0\n",
              "8   Breast Cancer Wisconsin          BCW  ...          9            0\n",
              "9     Pima Indians Diabetes          PID  ...          8            0\n",
              "10              Dermatology         DERM  ...         34            0\n",
              "11      Difficult Doughnut       Difdoug  ...         12            0\n",
              "12  4-Class Noisy Pinwheel           CNP  ...          2            0\n",
              "13                    Yeast        Yeast  ...          8            0\n",
              "14                     Spam         Spam  ...         57            0\n",
              "15                   Letter       Letter  ...         16            0\n",
              "\n",
              "[16 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr9jEzhwQ0Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerical = numerical[numerical['Abbreviation'] == 'Iris']\n",
        "numerical\n",
        "numerical = numerical.reset_index(drop=True)\n",
        "os.mkdir(\"numericals\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNfwf-rPhyW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NRMS_DICT = {}\n",
        "AE_DICT = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CusAfywyNjac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d299c54-0e37-45f3-efeb-36f362902638"
      },
      "source": [
        "import time\n",
        "begin_time = time.time()\n",
        "\n",
        "for index, row in tqdm(numerical.iterrows(), total=numerical.shape[0]): #For Each Numerical Dataset\n",
        "  subsets, original = read_subsets_and_original(BASE_PATH, ORIGINAL_BASE_PATH, row['Abbreviation']) # Get All Subsets and Original Dataset\n",
        "  subset_names = list(subsets.keys())\n",
        "  #ITERATE OVER ALL SUBSETS OF A DATASET AND APPLY GRNN ON EACH ONE\n",
        "  for each_subset_name in subset_names:\n",
        "\n",
        "    #SELECTING A SUBSET\n",
        "    selected_subset = subsets[each_subset_name]\n",
        "\n",
        "\n",
        "    new_prediction = np.zeros(shape=original.shape) #SAMPLE ARRAY TO SAVE PREDICTIONS\n",
        "    new_prediction = pd.DataFrame(data = new_prediction, columns=selected_subset.columns) \n",
        "\n",
        "\n",
        "    #COLUMNS ARRAY TO ITERATE\n",
        "    all_cols = np.array(original.columns) \n",
        "    for each in tqdm(all_cols, total=len(all_cols)):\n",
        "\n",
        "\n",
        "\n",
        "      #ONE COLUMN IN TEST AND OTHERS IN TRAINING\n",
        "      train_cols = all_cols[all_cols != each] \n",
        "      test_col = each\n",
        "\n",
        "      #CHECKING IF THERE ARE NULL VALUES IN OUR TEST COLUMNS\n",
        "      nulls = selected_subset[each].isnull() \n",
        "      test_index = nulls[nulls == True].index\n",
        "      train_index = nulls[nulls == False].index\n",
        "\n",
        "\n",
        "      #IF THERE IS NO NULL VALUE THEN WO WONT APPLY GRNN\n",
        "      if test_index.shape[0] == 0 or test_index.shape[0] / float(nulls.shape[0]) < 0.1:\n",
        "        new_prediction[each] = original[each].copy()\n",
        "\n",
        "      else:\n",
        "        ############################################\n",
        "        ############################################\n",
        "        #TRAIN GRNN ON INDEX WHERE THERE IS NO NULL AND PREDICT ON NULL VALUES\n",
        "        custom_GRNN = GRNN()\n",
        "        SAGA_BASED_FEATURES = SAGA_FEATURE_SELECTION(original[train_cols].loc[train_index], original[test_col].loc[train_index]) #SAGA\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "        #Normalization\n",
        "        normalizer = StandardScaler()\n",
        "\n",
        "        train_X = original[train_cols[SAGA_BASED_FEATURES]].loc[train_index].values\n",
        "        train_Y = original[test_col].loc[train_index].values\n",
        "\n",
        "        test_X = original[train_cols[SAGA_BASED_FEATURES]].loc[test_index].values\n",
        "\n",
        "        normalizer.fit(train_X, train_Y)\n",
        "\n",
        "        normalizer_train_X = normalizer.transform(train_X)\n",
        "        normalizer_test_X = normalizer.transform(test_X)\n",
        "        ############################################\n",
        "        ############################################\n",
        "        \n",
        "\n",
        "\n",
        "        custom_GRNN.fit(normalizer_train_X, train_Y)\n",
        "\n",
        "        #PREDICT\n",
        "        prediction_smothened = custom_GRNN.predict(normalizer_test_X)\n",
        "\n",
        "        #FILL OUR SAVING ARRAY WITH PREDICTIONS\n",
        "        new_prediction[each].loc[train_index] = selected_subset[each].loc[train_index]\n",
        "        new_prediction[each].loc[test_index] = prediction_smothened\n",
        "\n",
        "    new_prediction.to_csv(\"numericals/imputed_\" + each_subset_name + \".csv\", index=False)\n",
        "    NRMSE = calculate_NRMS(original.values, new_prediction.values)\n",
        "    log = log + \"Done Smoothing of : \" + each_subset_name + \" with NRMS : \" + str(NRMSE) + '/n/n/n'\n",
        "    NRMS_DICT[each_subset_name] = NRMSE\n",
        "\n",
        "end_time = time.time()\n",
        "diff = end_time - begin_time\n",
        "print(diff) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
            " 18%|█▊        | 8/44 [00:00<00:00, 79.31it/s]\u001b[A\n",
            " 34%|███▍      | 15/44 [00:00<00:00, 75.52it/s]\u001b[A\n",
            " 50%|█████     | 22/44 [00:00<00:00, 72.58it/s]\u001b[A\n",
            " 66%|██████▌   | 29/44 [00:00<00:00, 71.51it/s]\u001b[A\n",
            "100%|██████████| 44/44 [00:00<00:00, 71.96it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 70.78it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 329.09it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 68.07it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 317.01it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 95.03it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 59.53it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 114.61it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 45.11it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 304.97it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 57.95it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 88.19it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 70.49it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 259.75it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 71.20it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 65.66it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 88.14it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 341.17it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 62.86it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 61.48it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 329.73it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 82.44it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 52.22it/s]\n",
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 38.53it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 321.01it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 290.14it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 97.39it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 72.80it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 339.54it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 332.16it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 51.26it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 47.14it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 323.06it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 61.92it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 317.14it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 51.11it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 324.18it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 109.56it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 101.45it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 47.54it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 301.02it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 275.26it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 95.51it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 65.31it/s]\n",
            "\n",
            "100%|██████████| 4/4 [00:00<00:00, 341.51it/s]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.8924708366394043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15zKMjNgVDud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EycykrQIqOE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "zipf = zipfile.ZipFile('numericals.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('numericals/', zipf)\n",
        "zipf.close()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9wf8qltDpUB",
        "colab_type": "text"
      },
      "source": [
        "**Categorical Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCLVWnT5BwjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "16738319-cfbb-4a6c-d055-772c8da49aa8"
      },
      "source": [
        "categorical #Datasets with only Numerical Values\n",
        "categorical['Abbreviation'][categorical['Abbreviation'] == 'TTTEG'] = 'TTTTEG'\n",
        "categorical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset Name</th>\n",
              "      <th>Abbreviation</th>\n",
              "      <th>Instances</th>\n",
              "      <th>Features</th>\n",
              "      <th>Classes</th>\n",
              "      <th>Numerical</th>\n",
              "      <th>Categorical</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tic-Tac-Toe End game</td>\n",
              "      <td>TTTTEG</td>\n",
              "      <td>958</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>House Votes</td>\n",
              "      <td>HOV</td>\n",
              "      <td>234</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mushroom</td>\n",
              "      <td>MUSH</td>\n",
              "      <td>5644</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Splice</td>\n",
              "      <td>Splice</td>\n",
              "      <td>3190</td>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Connect-4</td>\n",
              "      <td>C4</td>\n",
              "      <td>67557</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Dataset Name Abbreviation  ...  Numerical  Categorical\n",
              "0  Tic-Tac-Toe End game       TTTTEG  ...          0            9\n",
              "1           House Votes          HOV  ...          0           16\n",
              "2              Mushroom         MUSH  ...          0           22\n",
              "3                Splice       Splice  ...          0           60\n",
              "4             Connect-4           C4  ...          0           42\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu7pY6LtIZTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical = categorical[categorical['Abbreviation'] == 'HOV']\n",
        "categorical\n",
        "categorical = categorical.reset_index(drop=True)\n",
        "os.mkdir(\"categorical\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2mVTmn7iKxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "9230b9a8-ea45-4a6a-dd42-42215549c940"
      },
      "source": [
        "import time\n",
        "begin_time = time.time()\n",
        "for index, row in tqdm(categorical.iterrows(), total=categorical.shape[0]): #For Each Numerical Dataset\n",
        "  subsets, original = read_subsets_and_original(BASE_PATH, ORIGINAL_BASE_PATH, row['Abbreviation']) # Get All Subsets and Original Dataset\n",
        "  new_columns = []\n",
        "  for each in original.columns:\n",
        "    x_each = str(each)\n",
        "    new_columns.append(x_each.replace(\" \", \"\"))\n",
        "\n",
        "  xoriginal = original.copy()\n",
        "  \n",
        "  subset_names = list(subsets.keys())\n",
        "  #ITERATE OVER ALL SUBSETS OF A DATASET AND APPLY GRNN ON EACH ONE\n",
        "  for each_subset_name in subset_names:\n",
        "\n",
        "    #SELECTING A SUBSET\n",
        "    selected_subset = subsets[each_subset_name]\n",
        "    ss = pd.concat([selected_subset, xoriginal])\n",
        "    ss = pd.get_dummies(ss)#APPLY ONE HOT ENCODING\n",
        "    selected_subset = ss[0:selected_subset.shape[0]]\n",
        "    original = ss[selected_subset.shape[0] :]\n",
        "    \n",
        "    new_columns = []\n",
        "    for each in original.columns:\n",
        "      x_each = str(each)\n",
        "      new_columns.append(x_each.replace(\" \", \"\"))\n",
        "    original.columns = new_columns\n",
        "\n",
        "\n",
        "    new_columns = []\n",
        "    for each in selected_subset.columns:\n",
        "      x_each = str(each)\n",
        "      new_columns.append(x_each.replace(\" \", \"\"))\n",
        "    selected_subset.columns = new_columns\n",
        "\n",
        "\n",
        "\n",
        "    new_prediction = np.zeros(shape=original.shape) #SAMPLE ARRAY TO SAVE PREDICTIONS\n",
        "    new_prediction = pd.DataFrame(data = new_prediction, columns=selected_subset.columns) \n",
        "\n",
        "\n",
        "    #COLUMNS ARRAY TO ITERATE\n",
        "    all_cols = np.array(original.columns) \n",
        "    for each in all_cols:\n",
        "\n",
        "\n",
        "\n",
        "      #ONE COLUMN IN TEST AND OTHERS IN TRAINING\n",
        "      train_cols = all_cols[all_cols != each] \n",
        "      test_col = each\n",
        "\n",
        "      #CHECKING IF THERE ARE NULL VALUES IN OUR TEST COLUMNS\n",
        "      nulls = selected_subset[each].isnull() \n",
        "      if len(nulls.shape) > 1:\n",
        "        if nulls.shape[1] > 1:\n",
        "          nulls = pd.DataFrame(nulls.values[:, 1], columns=[each])\n",
        "      test_index = nulls[nulls == True].index\n",
        "      train_index = nulls[nulls == False].index\n",
        "\n",
        "\n",
        "      #IF THERE IS NO NULL VALUE THEN WO WONT APPLY GRNN\n",
        "      if test_index.shape[0] == 0 or test_index.shape[0] / float(nulls.shape[0]) < 0.1:\n",
        "        new_prediction[each] = original[each].copy()\n",
        "\n",
        "      elif test_index.shape[0] == test_index.shape[0]:\n",
        "        new_prediction[each] = 0\n",
        "\n",
        "      else:\n",
        "        #TRAIN GRNN ON INDEX WHERE THERE IS NO NULL AND PREDICT ON NULL VALUES\n",
        "        custom_GRNN = GRNN()\n",
        "        custom_GRNN.fit(original[train_cols].loc[train_index].values, original[test_col].loc[train_index].values)\n",
        "\n",
        "        #PREDICT\n",
        "        prediction_smothened = custom_GRNN.predict(original[train_cols].loc[test_index].values)\n",
        "\n",
        "        #FILL OUR SAVING ARRAY WITH PREDICTIONS\n",
        "        new_prediction[each].loc[train_index] = selected_subset[each].loc[train_index]\n",
        "        if len(prediction_smothened.shape) > 1:\n",
        "          if prediction_smothened.shape[0] > 1 and prediction_smothened.shape[1] > 1:\n",
        "            prediction_smothened = prediction_smothened[0, :]\n",
        "            cols = new_prediction.columns\n",
        "            x = new_prediction.pop(each)\n",
        "            new_prediction[each] = x.values[:, 0]\n",
        "            new_prediction = new_prediction[cols]\n",
        "        \n",
        "        new_prediction[each].loc[test_index] = prediction_smothened\n",
        "\n",
        "    new_prediction.to_csv(\"categorical/imputed_\" + each_subset_name + \".csv\", index=False)\n",
        "    AE = calculate_AE_DICT(original.values, new_prediction.values)\n",
        "    log = log + \"Done Smoothing of : \" + each_subset_name + \" with AE : \" + str(AE) + '/n/n/n'\n",
        "    AE_DICT[each_subset_name] = AE\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "diff = end_time - begin_time\n",
        "print(diff) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/44 [00:00<00:20,  2.13it/s]\u001b[A\n",
            "  5%|▍         | 2/44 [00:00<00:19,  2.10it/s]\u001b[A\n",
            "  7%|▋         | 3/44 [00:01<00:19,  2.10it/s]\u001b[A\n",
            "  9%|▉         | 4/44 [00:01<00:20,  1.99it/s]\u001b[A\n",
            " 11%|█▏        | 5/44 [00:02<00:19,  1.99it/s]\u001b[A\n",
            " 14%|█▎        | 6/44 [00:03<00:20,  1.90it/s]\u001b[A\n",
            " 16%|█▌        | 7/44 [00:03<00:19,  1.92it/s]\u001b[A\n",
            " 18%|█▊        | 8/44 [00:04<00:17,  2.02it/s]\u001b[A\n",
            " 20%|██        | 9/44 [00:04<00:16,  2.09it/s]\u001b[A\n",
            " 23%|██▎       | 10/44 [00:04<00:15,  2.14it/s]\u001b[A\n",
            " 25%|██▌       | 11/44 [00:05<00:15,  2.17it/s]\u001b[A\n",
            " 27%|██▋       | 12/44 [00:05<00:14,  2.17it/s]\u001b[A\n",
            " 30%|██▉       | 13/44 [00:06<00:14,  2.18it/s]\u001b[A\n",
            " 32%|███▏      | 14/44 [00:06<00:13,  2.17it/s]\u001b[A\n",
            " 34%|███▍      | 15/44 [00:07<00:13,  2.18it/s]\u001b[A\n",
            " 36%|███▋      | 16/44 [00:07<00:12,  2.23it/s]\u001b[A\n",
            " 39%|███▊      | 17/44 [00:08<00:12,  2.22it/s]\u001b[A\n",
            " 41%|████      | 18/44 [00:08<00:11,  2.24it/s]\u001b[A\n",
            " 43%|████▎     | 19/44 [00:09<00:11,  2.14it/s]\u001b[A\n",
            " 45%|████▌     | 20/44 [00:09<00:11,  2.17it/s]\u001b[A\n",
            " 48%|████▊     | 21/44 [00:09<00:10,  2.20it/s]\u001b[A\n",
            " 50%|█████     | 22/44 [00:10<00:10,  2.10it/s]\u001b[A\n",
            " 52%|█████▏    | 23/44 [00:10<00:10,  2.06it/s]\u001b[A\n",
            " 55%|█████▍    | 24/44 [00:11<00:09,  2.12it/s]\u001b[A\n",
            " 57%|█████▋    | 25/44 [00:11<00:08,  2.12it/s]\u001b[A\n",
            " 59%|█████▉    | 26/44 [00:12<00:08,  2.14it/s]\u001b[A\n",
            " 61%|██████▏   | 27/44 [00:12<00:07,  2.17it/s]\u001b[A\n",
            " 64%|██████▎   | 28/44 [00:13<00:07,  2.17it/s]\u001b[A\n",
            " 66%|██████▌   | 29/44 [00:13<00:06,  2.21it/s]\u001b[A\n",
            " 68%|██████▊   | 30/44 [00:14<00:06,  2.06it/s]\u001b[A\n",
            " 70%|███████   | 31/44 [00:14<00:06,  2.12it/s]\u001b[A\n",
            " 73%|███████▎  | 32/44 [00:15<00:05,  2.15it/s]\u001b[A\n",
            " 75%|███████▌  | 33/44 [00:15<00:05,  2.19it/s]\u001b[A\n",
            " 77%|███████▋  | 34/44 [00:16<00:04,  2.15it/s]\u001b[A\n",
            " 80%|███████▉  | 35/44 [00:16<00:04,  2.21it/s]\u001b[A\n",
            " 82%|████████▏ | 36/44 [00:16<00:03,  2.24it/s]\u001b[A\n",
            " 84%|████████▍ | 37/44 [00:17<00:03,  2.21it/s]\u001b[A\n",
            " 86%|████████▋ | 38/44 [00:17<00:02,  2.23it/s]\u001b[A\n",
            " 89%|████████▊ | 39/44 [00:18<00:02,  2.05it/s]\u001b[A\n",
            " 91%|█████████ | 40/44 [00:18<00:02,  1.99it/s]\u001b[A\n",
            " 93%|█████████▎| 41/44 [00:19<00:01,  2.05it/s]\u001b[A\n",
            " 95%|█████████▌| 42/44 [00:19<00:01,  1.98it/s]\u001b[A\n",
            " 98%|█████████▊| 43/44 [00:20<00:00,  2.08it/s]\u001b[A\n",
            "100%|██████████| 44/44 [00:20<00:00,  2.12it/s]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25.23512864112854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO941sjGY0Qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k : k belongs to (1, n) #SAGA FEATURE SELECTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6qtU2Aksbt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipf = zipfile.ZipFile('categorical.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('categorical/', zipf)\n",
        "zipf.close()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3RgRIJREGjp",
        "colab_type": "text"
      },
      "source": [
        "**Combined Numerical and Categorical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi3Ywnm6qY7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined #Datasets with only Numerical Values\n",
        "combined['Abbreviation'][combined['Abbreviation'] == 'Credit'] = 'Credit'\n",
        "combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3gKXENjIsMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined = combined[combined['Abbreviation'] == 'Aheart']\n",
        "combined\n",
        "combined = combined.reset_index(drop=True)\n",
        "os.mkdir(\"combined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gglfhmfxD0e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "begin_time = time.time()\n",
        "for index, row in tqdm(combined.iterrows(), total=combined.shape[0]): #For Each Numerical Dataset\n",
        "  subsets, original = read_subsets_and_original(BASE_PATH, ORIGINAL_BASE_PATH, row['Abbreviation']) # Get All Subsets and Original Dataset\n",
        "  dts = original.dtypes\n",
        "  numerical_columns = dts[dts != 'O'].index.values\n",
        "  cat_columns = dts[dts == 'O'].index.values\n",
        "  original = original[numerical_columns.tolist() + cat_columns.tolist()]\n",
        "  xoriginal = original.copy()\n",
        "  \n",
        "  subset_names = list(subsets.keys())\n",
        "  #ITERATE OVER ALL SUBSETS OF A DATASET AND APPLY GRNN ON EACH ONE\n",
        "  for each_subset_name in subset_names:\n",
        "\n",
        "    #SELECTING A SUBSET\n",
        "    selected_subset = subsets[each_subset_name]\n",
        "    selected_subset = selected_subset[numerical_columns.tolist() + cat_columns.tolist()]\n",
        "\n",
        "    ss = pd.concat([selected_subset, xoriginal])\n",
        "    ss = pd.get_dummies(ss)#APPLY ONE HOT ENCODING\n",
        "    selected_subset = ss[0:selected_subset.shape[0]]\n",
        "    original = ss[selected_subset.shape[0] :]\n",
        "    \n",
        "    new_columns = []\n",
        "    for each in original.columns:\n",
        "      x_each = str(each)\n",
        "      new_columns.append(x_each.replace(\" \", \"\"))\n",
        "    original.columns = new_columns\n",
        "\n",
        "    new_columns = []\n",
        "    for each in selected_subset.columns:\n",
        "      x_each = str(each)\n",
        "      new_columns.append(x_each.replace(\" \", \"\"))\n",
        "    selected_subset.columns = new_columns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    new_prediction = np.zeros(shape=original.shape) #SAMPLE ARRAY TO SAVE PREDICTIONS\n",
        "    new_prediction = pd.DataFrame(data = new_prediction, columns=selected_subset.columns) \n",
        "\n",
        "\n",
        "    #COLUMNS ARRAY TO ITERATE\n",
        "    all_cols = np.array(original.columns) \n",
        "    for each in all_cols:\n",
        "\n",
        "\n",
        "\n",
        "      #ONE COLUMN IN TEST AND OTHERS IN TRAINING\n",
        "      train_cols = all_cols[all_cols != each] \n",
        "      test_col = each\n",
        "\n",
        "      #CHECKING IF THERE ARE NULL VALUES IN OUR TEST COLUMNS\n",
        "      nulls = selected_subset[each].isnull() \n",
        "      if len(nulls.shape) > 1:\n",
        "        if nulls.shape[1] > 1:\n",
        "          nulls = pd.DataFrame(nulls.values[:, 1], columns=[each])\n",
        "      test_index = nulls[nulls == True].index\n",
        "      train_index = nulls[nulls == False].index\n",
        "\n",
        "\n",
        "      #IF THERE IS NO NULL VALUE THEN WO WONT APPLY GRNN\n",
        "      if test_index.shape[0] == 0 or test_index.shape[0] / float(nulls.shape[0]) < 0.1:\n",
        "        new_prediction[each] = original[each].copy()\n",
        "\n",
        "      elif test_index.shape[0] == test_index.shape[0]:\n",
        "        new_prediction[each] = 0\n",
        "\n",
        "      else:\n",
        "        #TRAIN GRNN ON INDEX WHERE THERE IS NO NULL AND PREDICT ON NULL VALUES\n",
        "        custom_GRNN = GRNN()\n",
        "        custom_GRNN.fit(original[train_cols].loc[train_index].values, original[test_col].loc[train_index].values)\n",
        "\n",
        "        #PREDICT\n",
        "        prediction_smothened = custom_GRNN.predict(original[train_cols].loc[test_index].values)\n",
        "\n",
        "        #FILL OUR SAVING ARRAY WITH PREDICTIONS\n",
        "        new_prediction[each].loc[train_index] = selected_subset[each].loc[train_index]\n",
        "        if len(prediction_smothened.shape) > 1:\n",
        "          if prediction_smothened.shape[0] > 1 and prediction_smothened.shape[1] > 1:\n",
        "            prediction_smothened = prediction_smothened[0, :]\n",
        "            cols = new_prediction.columns\n",
        "            x = new_prediction.pop(each)\n",
        "            new_prediction[each] = x.values[:, 0]\n",
        "            new_prediction = new_prediction[cols]\n",
        "        \n",
        "\n",
        "        new_prediction[each].loc[test_index] = prediction_smothened\n",
        "\n",
        "\n",
        "\n",
        "    NRMSE = calculate_NRMS(original.values, new_prediction.values)\n",
        "    log = log + \"Done Smoothing of : \" + each_subset_name + \" with NRMS : \" + str(NRMSE) + '/n/n/n'\n",
        "    NRMS_DICT[each_subset_name] = NRMSE\n",
        "\n",
        "    ccat_cols = []\n",
        "    ocols = original.columns\n",
        "    for each_cc in ocols:\n",
        "      if each_cc not in numerical_columns.tolist():\n",
        "          ccat_cols.append(each_cc)\n",
        "    AE = calculate_AE_DICT(original[ccat_cols].values, new_prediction[ccat_cols].values)\n",
        "    log = log + \"Done Smoothing of : \" + each_subset_name + \" with AE : \" + str(AE) + '/n/n/n'\n",
        "    AE_DICT[each_subset_name] = AE  \n",
        "    new_prediction.to_csv(\"combined/imputed_\" + each_subset_name + \".csv\", index=False)\n",
        "\n",
        "end_time = time.time()\n",
        "diff = end_time - begin_time\n",
        "print(diff)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghFIlssNsYOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipf = zipfile.ZipFile('combined.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('combined/', zipf)\n",
        "zipf.close()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llSSGn9LL72J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "39120972-9cc5-4fa0-ef98-2c1e1ad23022"
      },
      "source": [
        "print (log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing warm start.../nWarm start concluded. The optimum isotropic sigma is [0.50951578]/nGradient search concluded. The optimum sigma is [0.5095154]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.24018485]/nGradient search concluded. The optimum sigma is [0.24]/nDone Smoothing of : Iris_AE_10 with NRMS : 0.023104781211727415/n/n/nDone Smoothing of : Iris_AE_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.06935079]/nGradient search concluded. The optimum sigma is [0.069]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.]/nGradient search concluded. The optimum sigma is [0.]/nDone Smoothing of : Iris_AE_20 with NRMS : 0.10312658936206778/n/n/nDone Smoothing of : Iris_AG_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.15651098]/nGradient search concluded. The optimum sigma is [0.15650383]/nDone Smoothing of : Iris_AE_5 with NRMS : 0.008558792880067568/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.4365855]/nGradient search concluded. The optimum sigma is [0.43660702]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.18333865]/nGradient search concluded. The optimum sigma is [0.18301693]/nDone Smoothing of : Iris_AG_10 with NRMS : 0.024773226313086552/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.13446438]/nGradient search concluded. The optimum sigma is [0.13443625]/nDone Smoothing of : Iris_AG_5 with NRMS : 0.010734792950770157/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.17533643]/nGradient search concluded. The optimum sigma is [0.17533655]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.0429436]/nGradient search concluded. The optimum sigma is [0.04294418]/nDone Smoothing of : Iris_AG_20 with NRMS : 0.028879151248921254/n/n/nDone Smoothing of : Iris_AL_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.21923057]/nGradient search concluded. The optimum sigma is [0.21923057]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.15446174]/nGradient search concluded. The optimum sigma is [0.15447076]/nDone Smoothing of : Iris_AL_10 with NRMS : 0.017746932092333526/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.47462699]/nGradient search concluded. The optimum sigma is [0.47462791]/nDone Smoothing of : Iris_AL_5 with NRMS : 0.014977727625477194/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.48015087]/nGradient search concluded. The optimum sigma is [0.48015281]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.32077947]/nGradient search concluded. The optimum sigma is [0.32098333]/nDone Smoothing of : Iris_AL_20 with NRMS : 0.03069802542507775/n/n/nDone Smoothing of : Iris_AN_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.44764326]/nGradient search concluded. The optimum sigma is [0.44763383]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.16339425]/nGradient search concluded. The optimum sigma is [0.16303915]/nDone Smoothing of : Iris_AN_10 with NRMS : 0.021967349052511218/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.56708696]/nGradient search concluded. The optimum sigma is [0.56702828]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.191699]/nGradient search concluded. The optimum sigma is [0.19198181]/nDone Smoothing of : Iris_AN_20 with NRMS : 0.04658013367597746/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.43229101]/nGradient search concluded. The optimum sigma is [0.43229206]/nDone Smoothing of : Iris_AN_5 with NRMS : 0.012310025866106203/n/n/nDone Smoothing of : Iris_AW_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.4227821]/nGradient search concluded. The optimum sigma is [0.4229841]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.12675457]/nGradient search concluded. The optimum sigma is [0.12696967]/nDone Smoothing of : Iris_AW_10 with NRMS : 0.016784334061853475/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.17550295]/nGradient search concluded. The optimum sigma is [0.17550347]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.06402998]/nGradient search concluded. The optimum sigma is [0.0640382]/nDone Smoothing of : Iris_AW_20 with NRMS : 0.03398181181607206/n/n/nDone Smoothing of : Iris_C_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.14116247]/nGradient search concluded. The optimum sigma is [0.14102527]/nDone Smoothing of : Iris_AW_5 with NRMS : 0.010335953270357775/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.50553636]/nGradient search concluded. The optimum sigma is [0.50553699]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.3057487]/nGradient search concluded. The optimum sigma is [0.30574049]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.14462744]/nGradient search concluded. The optimum sigma is [0.14497788]/nDone Smoothing of : Iris_C_10 with NRMS : 0.02249960461552534/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.15730453]/nGradient search concluded. The optimum sigma is [0.15730104]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.32713277]/nGradient search concluded. The optimum sigma is [0.32713651]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.34308242]/nGradient search concluded. The optimum sigma is [0.34307858]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.24489016]/nGradient search concluded. The optimum sigma is [0.245]/nDone Smoothing of : Iris_C_20 with NRMS : 0.03704537658551991/n/n/nDone Smoothing of : Iris_C_5 with NRMS : 0.0/n/n/nDone Smoothing of : Iris_NE_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.20659547]/nGradient search concluded. The optimum sigma is [0.20658978]/nDone Smoothing of : Iris_NE_10 with NRMS : 0.0216304972685188/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.56391166]/nGradient search concluded. The optimum sigma is [0.56391565]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.25140109]/nGradient search concluded. The optimum sigma is [0.25140572]/nDone Smoothing of : Iris_NE_20 with NRMS : 0.03807243254225203/n/n/nDone Smoothing of : Iris_NG_1 with NRMS : 0.0/n/n/nDone Smoothing of : Iris_NE_5 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.53933784]/nGradient search concluded. The optimum sigma is [0.539338]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.39699337]/nGradient search concluded. The optimum sigma is [0.397]/nDone Smoothing of : Iris_NG_10 with NRMS : 0.023286806655899835/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.52287147]/nGradient search concluded. The optimum sigma is [0.5228591]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.32961364]/nGradient search concluded. The optimum sigma is [0.32961373]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.36238259]/nGradient search concluded. The optimum sigma is [0.36238274]/nDone Smoothing of : Iris_NG_20 with NRMS : 0.03727162966925002/n/n/nDone Smoothing of : Iris_NG_5 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.56314499]/nGradient search concluded. The optimum sigma is [0.5631453]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.20590536]/nGradient search concluded. The optimum sigma is [0.20589431]/nDone Smoothing of : Iris_NL_10 with NRMS : 0.023780516594634135/n/n/nDone Smoothing of : Iris_NL_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.31336334]/nGradient search concluded. The optimum sigma is [0.3130238]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.40584808]/nGradient search concluded. The optimum sigma is [0.40596703]/nDone Smoothing of : Iris_NL_20 with NRMS : 0.03270968417561249/n/n/nDone Smoothing of : Iris_NN_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.38450175]/nGradient search concluded. The optimum sigma is [0.38450282]/nDone Smoothing of : Iris_NL_5 with NRMS : 0.01816572089020264/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.3107116]/nGradient search concluded. The optimum sigma is [0.31070783]/nDone Smoothing of : Iris_NN_10 with NRMS : 0.021137875843743936/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.62525959]/nGradient search concluded. The optimum sigma is [0.62525831]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.38418829]/nGradient search concluded. The optimum sigma is [0.38418996]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.18471126]/nGradient search concluded. The optimum sigma is [0.18497694]/nDone Smoothing of : Iris_NN_20 with NRMS : 0.03924533712967431/n/n/nDone Smoothing of : Iris_NN_5 with NRMS : 0.0/n/n/nDone Smoothing of : Iris_NW_1 with NRMS : 0.0/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.32558455]/nGradient search concluded. The optimum sigma is [0.32558174]/nDone Smoothing of : Iris_NW_10 with NRMS : 0.02943937289142972/n/n/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.61814081]/nGradient search concluded. The optimum sigma is [0.61813991]/nExecuting warm start.../nWarm start concluded. The optimum isotropic sigma is [0.0911456]/nGradient search concluded. The optimum sigma is [0.091]/nDone Smoothing of : Iris_NW_20 with NRMS : 0.04586368061363097/n/n/nDone Smoothing of : Iris_NW_5 with NRMS : 0.0/n/n/n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7kTZWXbTySr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_excel('gdrive/My Drive/Course Project Datasets/Course Project Datasets/Table.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f0L5gXmVlve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code to update and save\n",
        "logs = log.split('/n')\n",
        "for each in logs:\n",
        "    x = each.split(' ')\n",
        "    if len(x) > 0:\n",
        "        if x[0] == 'Done':\n",
        "            if x[6] == 'NRMS':\n",
        "                df['NRMS'][df['Datasets'] == str(x[4])] = float(x[8])\n",
        "            elif x[6] == 'AE':\n",
        "                df['AE'][df['Datasets'] == str(x[4])] = float(x[8])\n",
        "df.to_excel(\"Table.xlsx\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eVvF-Q1uLSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Analysis of the Project##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW5b3zR_uZcH",
        "colab_type": "text"
      },
      "source": [
        "Complete pipeline results in an upper bound of O(n^2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb_S_JvVuatt",
        "colab_type": "text"
      },
      "source": [
        "The model built is scalable to a large extent especially on parallel distributed computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-SJC8zjud2s",
        "colab_type": "text"
      },
      "source": [
        "HOV is the fastest data that was computed in 13.78 seconds whereas the letter dataset took\n",
        "2.75 hours to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru8PFul9ufrW",
        "colab_type": "text"
      },
      "source": [
        "The Sonar dataset has the least percentage of missing data (20%) and letter dataset had the\n",
        "highest percentage of missing data (80%). "
      ]
    }
  ]
}